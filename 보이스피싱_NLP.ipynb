{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "df759163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import audioread\n",
    "import numpy as np\n",
    "import resampy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "import warnings\n",
    "import urllib.request\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from konlpy.tag import Okt\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd33b19a",
   "metadata": {},
   "source": [
    "# 데이터 라벨링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "ec68433c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txt</th>\n",
       "      <th>wav</th>\n",
       "      <th>voice_fishing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>내가 나한테 잘해주면 돼요.혹시 여러분들 자식들가운데?지금 손자 손녀키운다고 먹고 ...</td>\n",
       "      <td>1-converted.ogg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>운명을 바꿔 놓습니다.서울대 음대를 나왔는데요.서울대가 중요한 게 아닙니다.결국 이...</td>\n",
       "      <td>10-converted.ogg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>네, 감사합니다.네, 감사합니다.서울대 면접없는 판 모국입니다 검사대 여름말씀이신건...</td>\n",
       "      <td>100-converted.ogg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>그러나 인류는 절대 내가 내 말에 절대 라는 용어를 여간 해서는 절대 안 된다는 얘...</td>\n",
       "      <td>11-converted.ogg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>그런데 외국사람이결국은 집 이사 이제 세방을 얻어 는데 그때 50 전세 한 달에 5...</td>\n",
       "      <td>12-converted.ogg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>회원 가입을 했거든요 하신다하는 건가요?혹시 방법을 높여야 하는지 잘 몰라서 보는데...</td>\n",
       "      <td>95-converted.ogg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>근데 이제 장소가 이제 구애가 없다고 하시면 저희가 그을지로 고용노동부건물에 있는 ...</td>\n",
       "      <td>96-converted.ogg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>감사합니다.확인 도와드릴 텐데 방송한 기다려 주시겠어요?여기야 기자 감사드리고요.거...</td>\n",
       "      <td>97-converted.ogg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>네 감사합니다.저 인사담당자신청했는데요 그 a니까 아직도 않았어요?네 감사합니다.</td>\n",
       "      <td>98-converted.ogg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>감사합니다.결과 지가 아직 않았어요.그래도 매일 두고 가 잘못돼서 그런 것 같아요....</td>\n",
       "      <td>99-converted.ogg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  txt                wav  \\\n",
       "0   내가 나한테 잘해주면 돼요.혹시 여러분들 자식들가운데?지금 손자 손녀키운다고 먹고 ...    1-converted.ogg   \n",
       "1   운명을 바꿔 놓습니다.서울대 음대를 나왔는데요.서울대가 중요한 게 아닙니다.결국 이...   10-converted.ogg   \n",
       "2   네, 감사합니다.네, 감사합니다.서울대 면접없는 판 모국입니다 검사대 여름말씀이신건...  100-converted.ogg   \n",
       "3   그러나 인류는 절대 내가 내 말에 절대 라는 용어를 여간 해서는 절대 안 된다는 얘...   11-converted.ogg   \n",
       "4   그런데 외국사람이결국은 집 이사 이제 세방을 얻어 는데 그때 50 전세 한 달에 5...   12-converted.ogg   \n",
       "..                                                ...                ...   \n",
       "95  회원 가입을 했거든요 하신다하는 건가요?혹시 방법을 높여야 하는지 잘 몰라서 보는데...   95-converted.ogg   \n",
       "96  근데 이제 장소가 이제 구애가 없다고 하시면 저희가 그을지로 고용노동부건물에 있는 ...   96-converted.ogg   \n",
       "97  감사합니다.확인 도와드릴 텐데 방송한 기다려 주시겠어요?여기야 기자 감사드리고요.거...   97-converted.ogg   \n",
       "98      네 감사합니다.저 인사담당자신청했는데요 그 a니까 아직도 않았어요?네 감사합니다.   98-converted.ogg   \n",
       "99  감사합니다.결과 지가 아직 않았어요.그래도 매일 두고 가 잘못돼서 그런 것 같아요....   99-converted.ogg   \n",
       "\n",
       "    voice_fishing  \n",
       "0               0  \n",
       "1               0  \n",
       "2               0  \n",
       "3               0  \n",
       "4               0  \n",
       "..            ...  \n",
       "95              0  \n",
       "96              0  \n",
       "97              0  \n",
       "98              0  \n",
       "99              0  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 일반대화 wav 데이터 불러오기 및 라벨링\n",
    "\n",
    "os.chdir('/Users/medici/Downloads/json')\n",
    "text_dir = '/Users/medici/Downloads/json/'\n",
    "\n",
    "voice_spam_text = [f for f in os.listdir()]\n",
    "# JSON 파일 열기\n",
    "d = []\n",
    "for k,file in enumerate(voice_spam_text):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    d.append('')\n",
    "    for i in range(len(data['Transcript'])):\n",
    "        try:\n",
    "            d[k]= d[k]+data['Transcript'][2*i-1]['Content']\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "\n",
    "d = pd.DataFrame(d)\n",
    "normal_dir = '/Users/medici/Downloads/일반대화 음성/'\n",
    "normal_files = os.listdir(normal_dir)\n",
    "d['wav'] = normal_files\n",
    "normal= d\n",
    "normal.rename(columns={0:'txt'},inplace=True)\n",
    "normal['voice_fishing']=0\n",
    "normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "109a6178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txt</th>\n",
       "      <th>wav</th>\n",
       "      <th>voice_fishing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>지금 전국적으로 약 이백 여명이 넘게 연료가 되어 있는 사건이고 일단 저희도 어느 ...</td>\n",
       "      <td>1-1-converted.ogg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>금융사기사건이었습니다네.직접 방문하셔서 개설하신.금융권은 어디 금융권들이 있으시죠?...</td>\n",
       "      <td>1-2-converted.ogg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>그럼 최근 삼 년 사이에 본인개인정보가 담긴 물품 분실하신 적 전혀 없으십니까?제가...</td>\n",
       "      <td>1-3-converted.ogg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>사실 있는 그대로만 진술하시면 되시는데 현재 계신 곳이 직장 이십니까.통합 불편하시...</td>\n",
       "      <td>1-4-converted.ogg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>일단 이 두 통장은 범죄로 사용돼서 추가적인 피해자가 발생하지 않도록 본인 의사와 ...</td>\n",
       "      <td>1-5-converted.ogg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>많은 분들이 연루돼 있는 부분들가운데서 저희가 지금 동시에 수사를 진행하다 보니까 ...</td>\n",
       "      <td>9-4.wav</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>이렇게 지금 전화 받아 가지고 있던 말아무도 본인이 지금 왜 보려는 전에 는지 본회...</td>\n",
       "      <td>9-5.wav</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>이제 최종 처리를 포함한 공범들하고는 이제 발견된 연관성이 없었기 때문에 일차적인 ...</td>\n",
       "      <td>9-6.wav</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>사건 으로 외모되셨으면 확인 한번 부탁드립니다.네 오세요.서울중앙지검에 김진수 아닙...</td>\n",
       "      <td>9-7.wav</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>일단 저희가 연락을 드린 이유가 혹시 본인께서 통장을 직접 개설하셔서 다른 제 삼자...</td>\n",
       "      <td>9-8.wav</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  txt                wav  \\\n",
       "0   지금 전국적으로 약 이백 여명이 넘게 연료가 되어 있는 사건이고 일단 저희도 어느 ...  1-1-converted.ogg   \n",
       "1   금융사기사건이었습니다네.직접 방문하셔서 개설하신.금융권은 어디 금융권들이 있으시죠?...  1-2-converted.ogg   \n",
       "2   그럼 최근 삼 년 사이에 본인개인정보가 담긴 물품 분실하신 적 전혀 없으십니까?제가...  1-3-converted.ogg   \n",
       "3   사실 있는 그대로만 진술하시면 되시는데 현재 계신 곳이 직장 이십니까.통합 불편하시...  1-4-converted.ogg   \n",
       "4   일단 이 두 통장은 범죄로 사용돼서 추가적인 피해자가 발생하지 않도록 본인 의사와 ...  1-5-converted.ogg   \n",
       "..                                                ...                ...   \n",
       "77  많은 분들이 연루돼 있는 부분들가운데서 저희가 지금 동시에 수사를 진행하다 보니까 ...            9-4.wav   \n",
       "78  이렇게 지금 전화 받아 가지고 있던 말아무도 본인이 지금 왜 보려는 전에 는지 본회...            9-5.wav   \n",
       "79  이제 최종 처리를 포함한 공범들하고는 이제 발견된 연관성이 없었기 때문에 일차적인 ...            9-6.wav   \n",
       "80  사건 으로 외모되셨으면 확인 한번 부탁드립니다.네 오세요.서울중앙지검에 김진수 아닙...            9-7.wav   \n",
       "81  일단 저희가 연락을 드린 이유가 혹시 본인께서 통장을 직접 개설하셔서 다른 제 삼자...            9-8.wav   \n",
       "\n",
       "    voice_fishing  \n",
       "0               1  \n",
       "1               1  \n",
       "2               1  \n",
       "3               1  \n",
       "4               1  \n",
       "..            ...  \n",
       "77              1  \n",
       "78              1  \n",
       "79              1  \n",
       "80              1  \n",
       "81              1  \n",
       "\n",
       "[82 rows x 3 columns]"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 보이스피싱 wav 데이터 불러오기 및 라벨링 \n",
    "\n",
    "os.chdir('/Users/medici/Downloads/보이스피싱 json')\n",
    "text_dir = '/Users/medici/Downloads/보이스피싱 json/'\n",
    "\n",
    "voice_spam_text = [f for f in os.listdir()]\n",
    "# JSON 파일 열기\n",
    "d = []\n",
    "for k,file in enumerate(voice_spam_text):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    d.append('')\n",
    "    for i in range(len(data['Transcript'])):\n",
    "        try:\n",
    "            d[k]= d[k]+data['Transcript'][2*i-1]['Content']\n",
    "        except IndexError:\n",
    "            pass\n",
    "d = pd.DataFrame(d)\n",
    "voice_dir = '/Users/medici/Downloads/보이스피싱 음성/'\n",
    "voice_files = os.listdir(voice_dir)\n",
    "d['wav'] = voice_files\n",
    "voice= d\n",
    "voice.rename(columns={0:'txt'},inplace=True)\n",
    "voice['voice_fishing']=1\n",
    "\n",
    "voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "d3a8a590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 일반대화와 보이스피싱 합치기\n",
    "df = pd.concat([voice,normal],axis=0)\n",
    "df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd2dfae",
   "metadata": {},
   "source": [
    "# 음성 데이터를 통한 보이스피싱 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "638e5d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wav 데이터 mfcc로 수치화\n",
    "\n",
    "voice_spam_dir = '/Users/medici/Downloads/보이스피싱 음성/'\n",
    "normal_dir = '/Users/medici/Downloads/일반대화 음성/'\n",
    "df['filepath']=''\n",
    "df['voice_feature']=''\n",
    "\n",
    "\n",
    "for i in range(len(df)):\n",
    "    if df.at[i,'voice_fishing']==1:\n",
    "        df.at[i,'filepath'] = os.path.join(voice_spam_dir, df.at[i,'wav'])\n",
    "        y, sr = librosa.load(df.at[i,'filepath'], sr=22050)\n",
    "        y = librosa.util.normalize(y)  # 음량 조정\n",
    "        mfccs1 = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
    "        mfccs_mean1 = np.mean(mfccs1, axis=1)\n",
    "        df.at[i,'voice_feature'] = mfccs_mean1\n",
    "\n",
    "    else:\n",
    "        df.at[i,'filepath'] = os.path.join(normal_dir, df.at[i,'wav'])\n",
    "        y, sr = librosa.load(df.at[i,'filepath'], sr=22050)\n",
    "        y = librosa.util.normalize(y)  # 음량 조정\n",
    "        mfccs1 = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
    "        mfccs_mean1 = np.mean(mfccs1, axis=1)\n",
    "        df.at[i,'voice_feature'] = mfccs_mean1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "8ca61bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest:\n",
      "Accuracy: 0.945054945054945\n",
      "Recall: 0.9090909090909091\n",
      " \n",
      "GradientBoosting:\n",
      "Accuracy: 0.9340659340659341\n",
      "Recall: 0.8863636363636364\n",
      " \n",
      "XGBoost:\n",
      "Accuracy: 0.9230769230769231\n",
      "Recall: 0.8863636363636364\n",
      " \n",
      "LightGBM:\n",
      "Accuracy: 0.9560439560439561\n",
      "Recall: 0.9318181818181818\n"
     ]
    }
   ],
   "source": [
    "# 음성 데이터 학습\n",
    "\n",
    "df_temp=df.drop(columns=['txt','wav','voice_fishing','filepath'])\n",
    "X = df_temp\n",
    "y = df['voice_fishing']\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.5,random_state= 46)\n",
    "X_train1=list(X_train['voice_feature'])\n",
    "X_test1=list(X_test['voice_feature'])\n",
    "y_train1 = list(y_train)\n",
    "y_test1 = list(y_test)\n",
    "\n",
    "# 음성 데이터 학습\n",
    "\n",
    "df_temp=df.drop(columns=['txt','wav','voice_fishing','filepath'])\n",
    "X = df_temp\n",
    "y = df['voice_fishing']\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.5,random_state= 18)\n",
    "X_train1=list(X_train['voice_feature'])\n",
    "X_test1=list(X_test['voice_feature'])\n",
    "y_train1 = list(y_train)\n",
    "y_test1 = list(y_test)\n",
    "\n",
    "\n",
    "# randomforest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "parameters1 = {\n",
    "    \"n_estimators\" : [10, 50, 1000, 2000],\n",
    "    \"max_features\" : [\"sqrt\", \"log2\"],\n",
    "    \"max_depth\" : [2,30, 50,70,100,150,200,300,400]\n",
    "}\n",
    "n_iter_search = 10\n",
    "rf_rgs1 = RandomizedSearchCV(\n",
    "    rf, \n",
    "    param_distributions=parameters1,\n",
    "    scoring=\"recall\",\n",
    "    n_jobs=-1, \n",
    "    random_state=42,\n",
    "    n_iter = n_iter_search\n",
    ")\n",
    "rf_rgs1.fit(X_train1, y_train1)\n",
    "y_pred_rf1 = rf_rgs1.predict(X_test1)\n",
    "print(\"RandomForest:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test1, y_pred_rf1))\n",
    "print(\"Recall:\", recall_score(y_test1, y_pred_rf1))\n",
    "print(' ')\n",
    "\n",
    "\n",
    "\n",
    "# gradient boosting\n",
    "gb = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "parameters={\n",
    "    'n_estimators': [1300,1500,1700], \n",
    "    'max_depth': [15,20,22],\n",
    "    'min_samples_leaf': [33,35,40],\n",
    "    'min_samples_split': [500,700,900],\n",
    "    'learning_rate': [0.2,0.1],   \n",
    "}  \n",
    "n_iter_search = 20\n",
    "gb_kf_rgs = RandomizedSearchCV(\n",
    "    gb, \n",
    "    param_distributions=parameters,\n",
    "    scoring=\"recall\",\n",
    "    n_jobs=-1, \n",
    "    random_state=42,\n",
    "    n_iter = n_iter_search\n",
    ")\n",
    "gb_kf_rgs.fit(X_train1, y_train1)\n",
    "y_pred_gb1 = gb_kf_rgs.predict(X_test1)\n",
    "print(\"GradientBoosting:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test1, y_pred_gb1))\n",
    "print(\"Recall:\", recall_score(y_test1, y_pred_gb1))\n",
    "print(' ')\n",
    "\n",
    "\n",
    "# xgboost\n",
    "xgb= XGBClassifier( n_estimators=100,n_jobs=-1)\n",
    "parameters ={\n",
    "     \"n_estimators\":[200,300,400],\n",
    "     \"learning_rate\":[0.3, 0.5, 1.0],\n",
    "     \"max_depth\" : [6,7],\n",
    "     \"gamma\" : [0.1, 0.15],\n",
    "     \"subsample\":[0.5, 0.6, 0.7],\n",
    "     \"colsample_bytree\":[0.3, 0.5, 1],\n",
    "}\n",
    "\n",
    "xgb_gs =GridSearchCV(\n",
    "    xgb, \n",
    "    param_grid=parameters,\n",
    "    scoring=\"recall\",\n",
    "    n_jobs=-1, \n",
    ")\n",
    "xgb_gs.fit(X_train1, y_train1)\n",
    "\n",
    "y_pred_xgb1 = xgb_gs.predict(X_test1)\n",
    "print(\"XGBoost:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test1, y_pred_xgb1))\n",
    "print(\"Recall:\", recall_score(y_test1, y_pred_xgb1))\n",
    "print(' ')\n",
    "\n",
    "# Lightgbm\n",
    "lgbm = LGBMClassifier( random_state=42, n_jobs=-1)\n",
    "\n",
    "parameters ={\n",
    "    \"n_estimators\":[100, 200, 300, 500, 1000],\n",
    "    \"learning_rate\":[0.01, 0.1, 0.2, 0.5, 1],\n",
    "\n",
    "    \"max_depth\" : [1,3,5,7],\n",
    "    \"min_split_gain\" : [0, 0.1, 0.2, 0.4, 0.5],\n",
    "\n",
    "    \"subsample\":[0.3, 0.5, 0.7, 0.9],\n",
    "    \"colsample_bytree\":[0.3, 0.5, 0.7, 0.8, 0.9],\n",
    "\n",
    "    \"reg_alpha\": [0, 0.01, 0.1, 0.5, 1, 10 ],\n",
    "    \"reg_lambda\" : [0.01, 0.1, 0.5, 1, 10 ]\n",
    "}\n",
    "n_iter_search = 50\n",
    "lgb_kf_rgs = RandomizedSearchCV(\n",
    "    lgbm, \n",
    "    param_distributions=parameters,\n",
    "    scoring=\"recall\",\n",
    "    n_jobs=-1, \n",
    "    random_state=42,\n",
    "    n_iter = n_iter_search\n",
    ")\n",
    "\n",
    "lgb_kf_rgs.fit(X_train1, y_train1)\n",
    "y_pred_lgbm = lgb_kf_rgs.predict(X_test1)\n",
    "print(\"LightGBM:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test1, y_pred_lgbm))\n",
    "print(\"Recall:\", recall_score(y_test1, y_pred_lgbm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "3f8c18ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ensemble Recall: 0.9024390243902439\n",
      "Accuracy: 0.9120879120879121\n"
     ]
    }
   ],
   "source": [
    "# 음성 모델 ensemble\n",
    "\n",
    "ensemble_wav = VotingClassifier(estimators=[('rf', rf_rgs1), ('gb', gb_kf_rgs), ('xgb', xgb_gs), ('lgbm', lgb_kf_rgs)], voting='soft')\n",
    "ensemble_wav.fit(X_train1, y_train1)\n",
    "y_pred1 = ensemble_wav.predict(X_test1)\n",
    "\n",
    "print(\"ensemble Recall:\", recall_score(y_test1, y_pred1))\n",
    "print(\"Accuracy:\", accuracy_score(y_test1, y_pred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44456bee",
   "metadata": {},
   "source": [
    "# 텍스트 데이터(TF-IDF)를 통한 보이스피싱 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "a798f59f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txt</th>\n",
       "      <th>wav</th>\n",
       "      <th>voice_fishing</th>\n",
       "      <th>filepath</th>\n",
       "      <th>voice_feature</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>지금 전국적으로 약 이백 여명이 넘게 연료가 되어 있는 사건이고 일단 저희도 어느 ...</td>\n",
       "      <td>1-1-converted.ogg</td>\n",
       "      <td>1</td>\n",
       "      <td>/Users/medici/Downloads/보이스피싱 음성/1-1-converted...</td>\n",
       "      <td>[-329.98514, 126.196266, -32.76896, 11.3803215...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.150224</td>\n",
       "      <td>0.049030</td>\n",
       "      <td>0.287916</td>\n",
       "      <td>0.235100</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051535</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.056934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>금융사기사건이었습니다네.직접 방문하셔서 개설하신.금융권은 어디 금융권들이 있으시죠?...</td>\n",
       "      <td>1-2-converted.ogg</td>\n",
       "      <td>1</td>\n",
       "      <td>/Users/medici/Downloads/보이스피싱 음성/1-2-converted...</td>\n",
       "      <td>[-316.93973, 157.07793, -23.155636, 8.142115, ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.476662</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.140148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.224003</td>\n",
       "      <td>0.18568</td>\n",
       "      <td>0.303455</td>\n",
       "      <td>0.169637</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 txt                wav  \\\n",
       "0  지금 전국적으로 약 이백 여명이 넘게 연료가 되어 있는 사건이고 일단 저희도 어느 ...  1-1-converted.ogg   \n",
       "1  금융사기사건이었습니다네.직접 방문하셔서 개설하신.금융권은 어디 금융권들이 있으시죠?...  1-2-converted.ogg   \n",
       "\n",
       "   voice_fishing                                           filepath  \\\n",
       "0              1  /Users/medici/Downloads/보이스피싱 음성/1-1-converted...   \n",
       "1              1  /Users/medici/Downloads/보이스피싱 음성/1-2-converted...   \n",
       "\n",
       "                                       voice_feature    0    1    2    3  \\\n",
       "0  [-329.98514, 126.196266, -32.76896, 11.3803215...  0.0  0.0  0.0  0.0   \n",
       "1  [-316.93973, 157.07793, -23.155636, 8.142115, ...  0.0  0.0  0.0  0.0   \n",
       "\n",
       "          4  ...        40        41        42        43       44        45  \\\n",
       "0  0.000000  ...  0.150224  0.049030  0.287916  0.235100  0.00000  0.000000   \n",
       "1  0.476662  ...  0.000000  0.140148  0.000000  0.224003  0.18568  0.303455   \n",
       "\n",
       "         46        47   48        49  \n",
       "0  0.000000  0.051535  0.0  0.056934  \n",
       "1  0.169637  0.000000  0.0  0.000000  \n",
       "\n",
       "[2 rows x 55 columns]"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# txt 데이터 TF-IDF로 수치화\n",
    "\n",
    "temp=list(df['txt'])\n",
    "\n",
    "df['txt_feature']=''\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=50,lowercase=False) # TF-IDF 상위 50개 추출\n",
    "for i in range(len(df)):\n",
    "    df.at[i,'txt_feature'] = vectorizer.fit_transform(temp)[i]\n",
    "\n",
    "a=pd.DataFrame()\n",
    "for i in range(len(df)):\n",
    "    b=pd.DataFrame(df.at[i,'txt_feature'].todense())\n",
    "    a=pd.concat([a,b],axis=0)\n",
    "a.reset_index(drop=True,inplace=True)\n",
    "df =pd.concat([df,a],axis=1)\n",
    "df=df.drop(columns='txt_feature')\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "98a28e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중요도 TOP50 :\n",
      "내가: 2.8421903228549468\n",
      "우리: 2.8082887711792655\n",
      "본인께서: 2.6541380913520074\n",
      "연락을: 2.5718999931150353\n",
      "이거: 2.5206066987274847\n",
      "저는: 2.4718165345580525\n",
      "직접: 2.4718165345580525\n",
      "그러니까: 2.4482860371478585\n",
      "됩니다: 2.4482860371478585\n",
      "오늘: 2.42529651892316\n",
      "본인이: 2.42529651892316\n",
      "그런데: 2.42529651892316\n",
      "되어: 2.402823663071101\n",
      "바로: 2.3382851419335298\n",
      "가지고: 2.3382851419335298\n",
      "어떻게: 2.297463147413275\n",
      "그런: 2.277660520117095\n",
      "그러면: 2.277660520117095\n",
      "많이: 2.2582424342599934\n",
      "본인: 2.2582424342599934\n",
      "하는: 2.2582424342599934\n",
      "대해서: 2.2582424342599934\n",
      "어떤: 2.239194239289299\n",
      "가지: 2.239194239289299\n",
      "이게: 2.220502106277147\n",
      "그냥: 2.220502106277147\n",
      "다른: 2.20215296760895\n",
      "있어요: 2.1841344621062717\n",
      "해서: 2.1841344621062717\n",
      "일단: 2.1841344621062717\n",
      "같은: 2.166434885006871\n",
      "혹시: 2.166434885006871\n",
      "있습니다: 2.11514159061932\n",
      "겁니다: 2.11514159061932\n",
      "근데: 2.0823517677963297\n",
      "되는: 2.0350988829457837\n",
      "이런: 2.0350988829457837\n",
      "하고: 2.0198314108149953\n",
      "거예요: 1.960990910792062\n",
      "한번: 1.960990910792062\n",
      "저희: 1.9054210596372512\n",
      "저희가: 1.865680730987737\n",
      "그리고: 1.8527773261518292\n",
      "이제: 1.7321493383632145\n",
      "이렇게: 1.7321493383632145\n",
      "그래서: 1.666191370571417\n",
      "제가: 1.5650952537000482\n",
      "때문에: 1.5555258026838976\n",
      "있는: 1.5181382706122772\n",
      "지금: 1.4909872815463263\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF 상위 50개 값 확인\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=50, lowercase=False)\n",
    "\n",
    "voice_tfidf = vectorizer.fit_transform(temp)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "indices = np.argsort(vectorizer.idf_)[::-1]\n",
    "top_n = 50\n",
    "top_features = [feature_names[i] for i in indices]\n",
    "top_scores = [vectorizer.idf_[i] for i in indices]\n",
    "print(\"중요도 TOP50 :\")\n",
    "for i in range(top_n):\n",
    "    print(f\"{top_features[i]}: {top_scores[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "3c7ae05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest:\n",
      "Accuracy: 0.9120879120879121\n",
      "Recall: 0.8478260869565217\n",
      " \n",
      "GradientBoosting:\n",
      "Accuracy: 0.945054945054945\n",
      "Recall: 0.8913043478260869\n",
      " \n",
      "XGBoost:\n",
      "Accuracy: 0.9120879120879121\n",
      "Recall: 0.8260869565217391\n",
      " \n",
      "LightGBM:\n",
      "Accuracy: 0.8901098901098901\n",
      "Recall: 0.8043478260869565\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 데이터 학습\n",
    "df_text=df.drop(columns=['txt','wav','filepath','voice_feature'])\n",
    "X = df_text.drop(columns=['voice_fishing'])\n",
    "y = df['voice_fishing']\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.5,random_state= 25)\n",
    "y_train = list(y_train)\n",
    "y_test = list(y_test)\n",
    "\n",
    "# randomforest\n",
    "rf_text = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "parameters1 = {\n",
    "    \"n_estimators\" : [10, 50, 1000, 2000],\n",
    "    \"max_features\" : [\"sqrt\", \"log2\"],\n",
    "    \"max_depth\" : [2,30, 50,70,100,150,200,300,400]\n",
    "}\n",
    "n_iter_search = 10\n",
    "rf_rgs2 = RandomizedSearchCV(\n",
    "    rf_text, \n",
    "    param_distributions=parameters1,\n",
    "    scoring=\"recall\",\n",
    "    n_jobs=-1, \n",
    "    random_state=42,\n",
    "    n_iter = n_iter_search\n",
    ")\n",
    "rf_rgs2.fit(X_train, y_train)\n",
    "y_pred_rf2 = rf_rgs2.predict(X_test)\n",
    "print(\"RandomForest:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf2))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_rf2))\n",
    "print(' ')\n",
    "\n",
    "\n",
    "\n",
    "# gradient boosting\n",
    "gb_text = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "parameters={\n",
    "    'n_estimators': [1300,1500,1700], \n",
    "    'max_depth': [15,20,22],\n",
    "    'min_samples_leaf': [33,35,40],\n",
    "    'min_samples_split': [500,700,900],\n",
    "    'learning_rate': [0.2,0.1],   \n",
    "}  \n",
    "n_iter_search = 20\n",
    "gb_kf_rgs2 = RandomizedSearchCV(\n",
    "    gb_text, \n",
    "    param_distributions=parameters,\n",
    "    scoring=\"recall\",\n",
    "    n_jobs=-1, \n",
    "    random_state=42,\n",
    "    n_iter = n_iter_search\n",
    ")\n",
    "gb_kf_rgs2.fit(X_train, y_train)\n",
    "y_pred_gb2 = gb_kf_rgs2.predict(X_test)\n",
    "print(\"GradientBoosting:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_gb2))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_gb2))\n",
    "print(' ')\n",
    "\n",
    "\n",
    "# xgboost\n",
    "xgb_text= XGBClassifier( n_estimators=100,n_jobs=-1)\n",
    "parameters ={\n",
    "     \"n_estimators\":[200,300,400],\n",
    "     \"learning_rate\":[0.3, 0.5, 1.0],\n",
    "     \"max_depth\" : [6,7],\n",
    "     \"gamma\" : [0.1, 0.15],\n",
    "     \"subsample\":[0.5, 0.6, 0.7],\n",
    "     \"colsample_bytree\":[0.3, 0.5, 1],\n",
    "}\n",
    "\n",
    "xgb_gs2 =GridSearchCV(\n",
    "    xgb_text, \n",
    "    param_grid=parameters,\n",
    "    scoring=\"recall\",\n",
    "    n_jobs=-1, \n",
    ")\n",
    "xgb_gs2.fit(X_train, y_train)\n",
    "\n",
    "y_pred_xgb2 = xgb_gs2.predict(X_test)\n",
    "print(\"XGBoost:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_xgb2))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_xgb2))\n",
    "print(' ')\n",
    "\n",
    "# Lightgbm\n",
    "lgbm_text = LGBMClassifier( random_state=42, n_jobs=-1)\n",
    "\n",
    "parameters ={\n",
    "    \"n_estimators\":[100, 200, 300, 500, 1000],\n",
    "    \"learning_rate\":[0.01, 0.1, 0.2, 0.5, 1],\n",
    "\n",
    "    \"max_depth\" : [1,3,5,7],\n",
    "    \"min_split_gain\" : [0, 0.1, 0.2, 0.4, 0.5],\n",
    "\n",
    "    \"subsample\":[0.3, 0.5, 0.7, 0.9],\n",
    "    \"colsample_bytree\":[0.3, 0.5, 0.7, 0.8, 0.9],\n",
    "\n",
    "    \"reg_alpha\": [0, 0.01, 0.1, 0.5, 1, 10 ],\n",
    "    \"reg_lambda\" : [0.01, 0.1, 0.5, 1, 10 ]\n",
    "}\n",
    "n_iter_search = 50\n",
    "lgb_kf_rgs2 = RandomizedSearchCV(\n",
    "    lgbm_text, \n",
    "    param_distributions=parameters,\n",
    "    scoring=\"recall\",\n",
    "    n_jobs=-1, \n",
    "    random_state=42,\n",
    "    n_iter = n_iter_search\n",
    ")\n",
    "\n",
    "lgb_kf_rgs2.fit(X_train, y_train)\n",
    "y_pred_lgbm2 = lgb_kf_rgs2.predict(X_test)\n",
    "print(\"LightGBM:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_lgbm2))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_lgbm2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "669d9e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ensemble Recall: 0.9047619047619048\n",
      "Accuracy: 0.9340659340659341\n"
     ]
    }
   ],
   "source": [
    "# 텍스트(TF-IDF) 모델 ensemble\n",
    "ensemble_txt = VotingClassifier(estimators=[('rf', rf_rgs2), ('gb', gb_kf_rgs2), ('xgb', xgb_gs2), ('lgbm', lgb_kf_rgs2)], voting='soft')\n",
    "ensemble_txt.fit(X_train, y_train)\n",
    "y_pred = ensemble_txt.predict(X_test)\n",
    "\n",
    "print(\"ensemble Recall:\", recall_score(y_test, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42542fbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5188a620",
   "metadata": {},
   "source": [
    "# 텍스트 데이터(word2vec)로 보이스피싱 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e164cdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b5ce3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "07d0d8b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txt</th>\n",
       "      <th>voice_fishing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>지금 전국적으로 약 이백 여명이 넘게 연료가 되어 있는 사건이고 일단 저희도 어느 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>금융사기사건이었습니다네.직접 방문하셔서 개설하신.금융권은 어디 금융권들이 있으시죠?...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>그럼 최근 삼 년 사이에 본인개인정보가 담긴 물품 분실하신 적 전혀 없으십니까?제가...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>사실 있는 그대로만 진술하시면 되시는데 현재 계신 곳이 직장 이십니까.통합 불편하시...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>일단 이 두 통장은 범죄로 사용돼서 추가적인 피해자가 발생하지 않도록 본인 의사와 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>회원 가입을 했거든요 하신다하는 건가요?혹시 방법을 높여야 하는지 잘 몰라서 보는데...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>근데 이제 장소가 이제 구애가 없다고 하시면 저희가 그을지로 고용노동부건물에 있는 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>감사합니다.확인 도와드릴 텐데 방송한 기다려 주시겠어요?여기야 기자 감사드리고요.거...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>네 감사합니다.저 인사담당자신청했는데요 그 a니까 아직도 않았어요?네 감사합니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>감사합니다.결과 지가 아직 않았어요.그래도 매일 두고 가 잘못돼서 그런 것 같아요....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>182 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   txt  voice_fishing\n",
       "0    지금 전국적으로 약 이백 여명이 넘게 연료가 되어 있는 사건이고 일단 저희도 어느 ...              1\n",
       "1    금융사기사건이었습니다네.직접 방문하셔서 개설하신.금융권은 어디 금융권들이 있으시죠?...              1\n",
       "2    그럼 최근 삼 년 사이에 본인개인정보가 담긴 물품 분실하신 적 전혀 없으십니까?제가...              1\n",
       "3    사실 있는 그대로만 진술하시면 되시는데 현재 계신 곳이 직장 이십니까.통합 불편하시...              1\n",
       "4    일단 이 두 통장은 범죄로 사용돼서 추가적인 피해자가 발생하지 않도록 본인 의사와 ...              1\n",
       "..                                                 ...            ...\n",
       "177  회원 가입을 했거든요 하신다하는 건가요?혹시 방법을 높여야 하는지 잘 몰라서 보는데...              0\n",
       "178  근데 이제 장소가 이제 구애가 없다고 하시면 저희가 그을지로 고용노동부건물에 있는 ...              0\n",
       "179  감사합니다.확인 도와드릴 텐데 방송한 기다려 주시겠어요?여기야 기자 감사드리고요.거...              0\n",
       "180      네 감사합니다.저 인사담당자신청했는데요 그 a니까 아직도 않았어요?네 감사합니다.              0\n",
       "181  감사합니다.결과 지가 아직 않았어요.그래도 매일 두고 가 잘못돼서 그런 것 같아요....              0\n",
       "\n",
       "[182 rows x 2 columns]"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 일반대화와 보이스피싱 합치기\n",
    "df_sum = pd.concat([voice,normal])\n",
    "df_sum = df_sum.drop(columns='wav')\n",
    "df_sum= df_sum.reset_index(drop=True)\n",
    "df_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "116029b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 정의\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다','을','요','.','께서','에서','이다','되다']\n",
    "\n",
    "# 토큰화 \n",
    "okt = Okt()\n",
    "tokenized_data = []\n",
    "for sentence in df_sum['txt']:\n",
    "    temp_X = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "    tokenized_data.append(temp_X)\n",
    "\n",
    "# Word2vec 모델 학습\n",
    "model = Word2Vec(sentences = tokenized_data, window = 5, min_count = 1, workers = 4, sg = 0)\n",
    "model.wv.vectors.shape\n",
    "\n",
    "# 각 문장의 벡터 표현 생성\n",
    "sentence_vectors = []\n",
    "for text in tokenized_data:\n",
    "    vector = np.mean([model.wv[word] for word in text], axis=0)\n",
    "    sentence_vectors.append(vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "bbe2c096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest:\n",
      "Accuracy: 0.9010989010989011\n",
      "Recall: 0.8837209302325582\n",
      " \n",
      "GradientBoosting:\n",
      "Accuracy: 0.9340659340659341\n",
      "Recall: 0.9534883720930233\n",
      " \n",
      "XGBoost:\n",
      "Accuracy: 0.9230769230769231\n",
      "Recall: 0.9069767441860465\n",
      " \n",
      "LightGBM:\n",
      "Accuracy: 0.9340659340659341\n",
      "Recall: 0.9302325581395349\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 데이터(work2vec) 모델 학습\n",
    "\n",
    "labels = list(df_sum['voice_fishing'])\n",
    "X_train2,X_test2,y_train2,y_test2 = train_test_split(sentence_vectors,labels,test_size = 0.5,random_state= 24)\n",
    "\n",
    "# randomforest\n",
    "rf_wv = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "parameters1 = {\n",
    "    \"n_estimators\" : [10, 50, 1000, 2000],\n",
    "    \"max_features\" : [\"sqrt\", \"log2\"],\n",
    "    \"max_depth\" : [2,30, 50,70,100,150,200,300,400]\n",
    "}\n",
    "n_iter_search = 10\n",
    "rf_rgs3 = RandomizedSearchCV(\n",
    "    rf_wv, \n",
    "    param_distributions=parameters1,\n",
    "    scoring=\"recall\",\n",
    "    n_jobs=-1, \n",
    "    random_state=42,\n",
    "    n_iter = n_iter_search\n",
    ")\n",
    "rf_rgs3.fit(X_train2, y_train2)\n",
    "y_pred_rf3 = rf_rgs3.predict(X_test2)\n",
    "print(\"RandomForest:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test2, y_pred_rf3))\n",
    "print(\"Recall:\", recall_score(y_test2, y_pred_rf3))\n",
    "print(' ')\n",
    "\n",
    "\n",
    "\n",
    "# gradient boosting\n",
    "gb_wv = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "parameters={\n",
    "    'n_estimators': [1300,1500,1700], \n",
    "    'max_depth': [15,20,22],\n",
    "    'min_samples_leaf': [33,35,40],\n",
    "    'min_samples_split': [500,700,900],\n",
    "    'learning_rate': [0.2,0.1],   \n",
    "}  \n",
    "n_iter_search = 20\n",
    "gb_kf_rgs3 = RandomizedSearchCV(\n",
    "    gb_wv, \n",
    "    param_distributions=parameters,\n",
    "    scoring=\"recall\",\n",
    "    n_jobs=-1, \n",
    "    random_state=42,\n",
    "    n_iter = n_iter_search\n",
    ")\n",
    "gb_kf_rgs3.fit(X_train2, y_train2)\n",
    "y_pred_gb3 = gb_kf_rgs3.predict(X_test2)\n",
    "print(\"GradientBoosting:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test2, y_pred_gb3))\n",
    "print(\"Recall:\", recall_score(y_test2, y_pred_gb3))\n",
    "print(' ')\n",
    "\n",
    "\n",
    "# xgboost\n",
    "xgb_wv= XGBClassifier( n_estimators=100,n_jobs=-1)\n",
    "parameters ={\n",
    "     \"n_estimators\":[200,300,400],\n",
    "     \"learning_rate\":[0.3, 0.5, 1.0],\n",
    "     \"max_depth\" : [6,7],\n",
    "     \"gamma\" : [0.1, 0.15],\n",
    "     \"subsample\":[0.5, 0.6, 0.7],\n",
    "     \"colsample_bytree\":[0.3, 0.5, 1],\n",
    "}\n",
    "\n",
    "xgb_gs3 =GridSearchCV(\n",
    "    xgb_wv, \n",
    "    param_grid=parameters,\n",
    "    scoring=\"recall\",\n",
    "    n_jobs=-1, \n",
    ")\n",
    "xgb_gs3.fit(X_train2, y_train2)\n",
    "\n",
    "y_pred_xgb3 = xgb_gs3.predict(X_test2)\n",
    "print(\"XGBoost:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test2, y_pred_xgb3))\n",
    "print(\"Recall:\", recall_score(y_test2, y_pred_xgb3))\n",
    "print(' ')\n",
    "\n",
    "# Lightgbm\n",
    "lgbm_wv = LGBMClassifier( random_state=42, n_jobs=-1)\n",
    "\n",
    "parameters ={\n",
    "    \"n_estimators\":[100, 200, 300, 500, 1000],\n",
    "    \"learning_rate\":[0.01, 0.1, 0.2, 0.5, 1],\n",
    "\n",
    "    \"max_depth\" : [1,3,5,7],\n",
    "    \"min_split_gain\" : [0, 0.1, 0.2, 0.4, 0.5],\n",
    "\n",
    "    \"subsample\":[0.3, 0.5, 0.7, 0.9],\n",
    "    \"colsample_bytree\":[0.3, 0.5, 0.7, 0.8, 0.9],\n",
    "\n",
    "    \"reg_alpha\": [0, 0.01, 0.1, 0.5, 1, 10 ],\n",
    "    \"reg_lambda\" : [0.01, 0.1, 0.5, 1, 10 ]\n",
    "}\n",
    "n_iter_search = 50\n",
    "lgb_kf_rgs3 = RandomizedSearchCV(\n",
    "    lgbm_wv, \n",
    "    param_distributions=parameters,\n",
    "    scoring=\"recall\",\n",
    "    n_jobs=-1, \n",
    "    random_state=42,\n",
    "    n_iter = n_iter_search\n",
    ")\n",
    "\n",
    "lgb_kf_rgs3.fit(X_train2, y_train2)\n",
    "y_pred_lgbm3 = lgb_kf_rgs3.predict(X_test2)\n",
    "print(\"LightGBM:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test2, y_pred_lgbm3))\n",
    "print(\"Recall:\", recall_score(y_test2, y_pred_lgbm3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "99c86c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ensemble Recall: 0.9302325581395349\n",
      "Accuracy: 0.9120879120879121\n"
     ]
    }
   ],
   "source": [
    "# 텍스트(work2vec) 모델 ensemble\n",
    "ensemble_wv = VotingClassifier(estimators=[('rf', rf_rgs3), ('gb', gb_kf_rgs3), ('xgb', xgb_gs3), ('lgbm', lgb_kf_rgs3)], voting='soft')\n",
    "ensemble_wv.fit(X_train2, y_train2)\n",
    "y_pred3 = ensemble_wv.predict(X_test2)\n",
    "\n",
    "print(\"ensemble Recall:\", recall_score(y_test2, y_pred3))\n",
    "print(\"Accuracy:\", accuracy_score(y_test2, y_pred3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cfb9e8",
   "metadata": {},
   "source": [
    "# 음성과 텍스트(TF-IDF, word2vec)을 모두 활용한 보이스피싱 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "618b4d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest:\n",
      "Accuracy: 0.9454545454545454\n",
      "Recall: 0.9629629629629629\n",
      " \n",
      "GradientBoosting:\n",
      "Accuracy: 0.8727272727272727\n",
      "Recall: 0.8148148148148148\n",
      " \n",
      "XGBoost:\n",
      "Accuracy: 0.9454545454545454\n",
      "Recall: 0.9259259259259259\n",
      " \n",
      "LightGBM:\n",
      "Accuracy: 0.9090909090909091\n",
      "Recall: 0.8518518518518519\n"
     ]
    }
   ],
   "source": [
    "# 음성 feature, 텍스트(TF-IDF,word2vec) feature 결합\n",
    "\n",
    "voice_data_train = pd.DataFrame(X_train1)\n",
    "word2vec_data_train = pd.DataFrame(X_train2)\n",
    "tfidf_data_train = pd.DataFrame(X_train)\n",
    "voice_data_train.reset_index(drop=True,inplace=True)\n",
    "word2vec_data_train.reset_index(drop=True,inplace=True)\n",
    "tfidf_data_train.reset_index(drop=True,inplace=True)\n",
    "X_train3 = pd.concat([voice_data_train,word2vec_data_train],axis=1)\n",
    "X_train3 = pd.concat([X_train3,tfidf_data_train],axis=1)\n",
    "\n",
    "ss = list(range(191))\n",
    "X_train3.columns=ss\n",
    "X_test3.columns=ss\n",
    "\n",
    "w2v = pd.DataFrame(sentence_vectors)\n",
    "df_all = pd.concat([df,w2v],axis=1)\n",
    "df_all = df_all.drop(columns=['txt','wav','filepath'])\n",
    "X_train3 = df_all.drop(columns='voice_fishing')\n",
    "\n",
    "# 음성과 텍스트 동시에 학습\n",
    "\n",
    "# randomforest\n",
    "rf_all = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "parameters1 = {\n",
    "    \"n_estimators\" : [10, 50, 1000, 2000],\n",
    "    \"max_features\" : [\"sqrt\", \"log2\"],\n",
    "    \"max_depth\" : [2,30, 50,70,100,150,200,300,400]\n",
    "}\n",
    "n_iter_search = 10\n",
    "rf_rgs4 = RandomizedSearchCV(\n",
    "    rf_all, \n",
    "    param_distributions=parameters1,\n",
    "    scoring=\"recall\",\n",
    "    n_jobs=-1, \n",
    "    random_state=42,\n",
    "    n_iter = n_iter_search\n",
    ")\n",
    "rf_rgs4.fit(X_train3, y_train3)\n",
    "y_pred_rf4 = rf_rgs4.predict(X_test3)\n",
    "print(\"RandomForest:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test3, y_pred_rf4))\n",
    "print(\"Recall:\", recall_score(y_test3, y_pred_rf4))\n",
    "print(' ')\n",
    "\n",
    "\n",
    "\n",
    "# gradient boosting\n",
    "gb_all = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "parameters={\n",
    "    'n_estimators': [1300,1500,1700], \n",
    "    'max_depth': [15,20,22],\n",
    "    'min_samples_leaf': [33,35,40],\n",
    "    'min_samples_split': [500,700,900],\n",
    "    'learning_rate': [0.2,0.1],   \n",
    "}  \n",
    "n_iter_search = 20\n",
    "gb_kf_rgs4 = RandomizedSearchCV(\n",
    "    gb_all, \n",
    "    param_distributions=parameters,\n",
    "    scoring=\"recall\",\n",
    "    n_jobs=-1, \n",
    "    random_state=42,\n",
    "    n_iter = n_iter_search\n",
    ")\n",
    "gb_kf_rgs4.fit(X_train3, y_train3)\n",
    "y_pred_gb4 = gb_kf_rgs4.predict(X_test3)\n",
    "print(\"GradientBoosting:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test3, y_pred_gb4))\n",
    "print(\"Recall:\", recall_score(y_test3, y_pred_gb4))\n",
    "print(' ')\n",
    "\n",
    "\n",
    "# xgboost\n",
    "xgb_all= XGBClassifier( n_estimators=100,n_jobs=-1)\n",
    "parameters ={\n",
    "     \"n_estimators\":[200,300,400],\n",
    "     \"learning_rate\":[0.3, 0.5, 1.0],\n",
    "     \"max_depth\" : [6,7],\n",
    "     \"gamma\" : [0.1, 0.15],\n",
    "     \"subsample\":[0.5, 0.6, 0.7],\n",
    "     \"colsample_bytree\":[0.3, 0.5, 1],\n",
    "}\n",
    "\n",
    "xgb_gs4 =GridSearchCV(\n",
    "    xgb_all, \n",
    "    param_grid=parameters,\n",
    "    scoring=\"recall\",\n",
    "    n_jobs=-1, \n",
    ")\n",
    "xgb_gs4.fit(X_train3, y_train3)\n",
    "\n",
    "y_pred_xgb4 = xgb_gs4.predict(X_test3)\n",
    "print(\"XGBoost:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test3, y_pred_xgb4))\n",
    "print(\"Recall:\", recall_score(y_test3, y_pred_xgb4))\n",
    "print(' ')\n",
    "\n",
    "# Lightgbm\n",
    "lgbm_all = LGBMClassifier( random_state=42, n_jobs=-1)\n",
    "\n",
    "parameters ={\n",
    "    \"n_estimators\":[100, 200, 300, 500, 1000],\n",
    "    \"learning_rate\":[0.01, 0.1, 0.2, 0.5, 1],\n",
    "\n",
    "    \"max_depth\" : [1,3,5,7],\n",
    "    \"min_split_gain\" : [0, 0.1, 0.2, 0.4, 0.5],\n",
    "\n",
    "    \"subsample\":[0.3, 0.5, 0.7, 0.9],\n",
    "    \"colsample_bytree\":[0.3, 0.5, 0.7, 0.8, 0.9],\n",
    "\n",
    "    \"reg_alpha\": [0, 0.01, 0.1, 0.5, 1, 10 ],\n",
    "    \"reg_lambda\" : [0.01, 0.1, 0.5, 1, 10 ]\n",
    "}\n",
    "n_iter_search = 50\n",
    "lgb_kf_rgs4 = RandomizedSearchCV(\n",
    "    lgbm_all, \n",
    "    param_distributions=parameters,\n",
    "    scoring=\"recall\",\n",
    "    n_jobs=-1, \n",
    "    random_state=42,\n",
    "    n_iter = n_iter_search\n",
    ")\n",
    "\n",
    "lgb_kf_rgs4.fit(X_train3, y_train3)\n",
    "y_pred_lgbm4 = lgb_kf_rgs4.predict(X_test3)\n",
    "print(\"LightGBM:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test3, y_pred_lgbm4))\n",
    "print(\"Recall:\", recall_score(y_test3, y_pred_lgbm4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "e34ca077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ensemble Recall: 0.9318181818181818\n",
      "Accuracy: 0.967032967032967\n"
     ]
    }
   ],
   "source": [
    "# 음성과 텍스트 동시 모델 ensemble\n",
    "ensemble_f = VotingClassifier(estimators=[('rf', rf_rgs4), ('gb', gb_kf_rgs4), ('xgb', xgb_gs4), ('lgbm', lgb_kf_rgs4)], voting='soft')\n",
    "ensemble_f.fit(X_train3, y_train3)\n",
    "y_pred3 = ensemble_f.predict(X_test3)\n",
    "\n",
    "print(\"ensemble Recall:\", recall_score(y_test3, y_pred3))\n",
    "print(\"Accuracy:\", accuracy_score(y_test3, y_pred3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
